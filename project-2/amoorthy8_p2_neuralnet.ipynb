{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amoorthy8-p2-neuralnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "c6BaX10f0VCM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup code \n",
        "Imports and library installation and stuff! so fun!"
      ]
    },
    {
      "metadata": {
        "id": "TPVO3xUW0SVJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#############################\n",
        "# Library Installation Cell #\n",
        "#############################\n",
        "\n",
        "# !pip install mlrose\n",
        "!pip uninstall -y mlrose\n",
        "!pip install git+https://github.com/pipsqueaker/mlrose.git@master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y6vetfVhz8oF",
        "colab_type": "code",
        "outputId": "55599050-1fd2-4706-9fdd-b978d19ffe03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import mlrose\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pylab\n",
        "import copy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import os.path\n",
        "import scipy\n",
        "import sklearn\n",
        "import keras\n",
        "import pylab\n",
        "from keras.utils import np_utils \n",
        "try:\n",
        "    from google.colab import files\n",
        "except ImportError:\n",
        "  print(\"Not running on colab\")\n",
        "  \n",
        "def jagged_average(history_list):\n",
        "    max_history_length = max([h.size for h in history_list])\n",
        "    extended_histories = np.array([np.append(h, [np.nan] * (max_history_length - h.size)) \n",
        "                          for h in history_list])\n",
        "    \n",
        "    history_mask = extended_histories.copy()\n",
        "    history_mask[~np.isnan(history_mask)] = 1\n",
        "    extended_histories[np.isnan(extended_histories)] = 0\n",
        "    \n",
        "    return np.sum(extended_histories, axis=0) / np.sum(history_mask, axis=0)\n",
        "  \n",
        "RHC_metadict = []\n",
        "SA_metadict = []\n",
        "GA_metadict = []"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "87Or_RcY1Es2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#########################\n",
        "# Load CIFAR-10 Dataset #\n",
        "#########################\n",
        "\n",
        "\n",
        "#################################################################\n",
        "# Configurable parameters (don't look at anything outside here) #\n",
        "#################################################################\n",
        "\n",
        "VAL_OVER_TRAIN = .2\n",
        "TEST_OVER_TOTAL = .2\n",
        "DATA_FRACTION = .5\n",
        "\n",
        "#########################################\n",
        "# End stuff you should need to look at! #\n",
        "#########################################\n",
        "\n",
        "ONEHOTS_FILENAME = \"imdb-onehots.gz\"\n",
        "LABELS_FILENAME = \"imdb-labels.gz\"\n",
        "\n",
        "# Changeable parameters\n",
        "MAX_WORD_FEATURES = 10000\n",
        "\n",
        "def clean_text(raw_review):\n",
        "    # Function to convert a raw review to a string of words\n",
        "    \n",
        "    # Import modules\n",
        "    from bs4 import BeautifulSoup\n",
        "    import re\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem.porter import PorterStemmer\n",
        "    \n",
        "    review_text = BeautifulSoup(raw_review, 'html.parser').get_text() # Remove HTML\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) # Remove non-letters \n",
        "    words = letters_only.lower().split() # Convert to lower case, split into individual words\n",
        "    stops = set(stopwords.words(\"english\")) # Remove stop words (use of sets makes this faster)               \n",
        "    meaningful_words = [w for w in words if not w in stops]                             \n",
        "    porter = PorterStemmer() # Reduce word to stem of word\n",
        "    stemmed_words = [porter.stem(w) for w in meaningful_words]\n",
        "    joined_words = ( \" \".join( stemmed_words )) # Join the words back into one string separated by space\n",
        "    return joined_words \n",
        "\n",
        "def apply_cleaning_function_to_series(X):\n",
        "    print('Cleaning data')\n",
        "    start_time = time.time()\n",
        "    cleaned_X = []\n",
        "    for element in X:\n",
        "        cleaned_X.append(clean_text(element))\n",
        "    print ('Finished in ', str((time.time() - start_time)/60), \" minutes\")\n",
        "    return cleaned_X\n",
        "\n",
        "if not os.path.isfile(ONEHOTS_FILENAME + \".npz\"):\n",
        "    nltk.download('stopwords')\n",
        "    print(\"one-hots not created yet: cleaning and saving to file\")\n",
        "    print(\"Expect this to take about 10-15 minutes\")\n",
        "    data = pd.read_csv('https://gitlab.com/michaelallen1966/00_python_snippets_and_recipes/raw/master/machine_learning/data/IMDb.csv')\n",
        "    \n",
        "    x_cleaned = apply_cleaning_function_to_series(data[\"review\"])\n",
        "    all_y = np.array(data[\"sentiment\"]).ravel()\n",
        "    \n",
        "    # Free up memory!\n",
        "    data = None\n",
        "    vectorizer = CountVectorizer(analyzer=\"word\",\n",
        "                                 tokenizer=None,\n",
        "                                 preprocessor=None,\n",
        "                                 stop_words=None,\n",
        "                                 ngram_range=(1,1),\n",
        "                                 max_features=MAX_WORD_FEATURES)\n",
        "    vectorizer.fit(x_cleaned)\n",
        "    x_all = vectorizer.transform(x_cleaned)\n",
        "    x_cleaned = None\n",
        "    scipy.sparse.save_npz(ONEHOTS_FILENAME, x_all)\n",
        "    np.savetxt(LABELS_FILENAME, labels)\n",
        "else:\n",
        "    print(\"loading one-hots from file\")\n",
        "    start_time = time.time()\n",
        "    all_x = scipy.sparse.load_npz(ONEHOTS_FILENAME + \".npz\")\n",
        "    all_y = np.loadtxt(LABELS_FILENAME)\n",
        "    end_time = time.time()\n",
        "    print(\"Finished loading one-hots in \", (end_time - start_time)/60, \" minutes\")\n",
        "    \n",
        "cutoff_len = int(DATA_FRACTION * all_x.shape[0])\n",
        "all_x = all_x[:cutoff_len].toarray()\n",
        "all_y = all_y[:cutoff_len]\n",
        "# files.upload()\n",
        "print(\"All X shape: \", all_x.shape)\n",
        "print(\"All y shape: \", all_y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hFAPhL-nIJpx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# all_x, all_y = np.concatenate([x_train, x_test]), np.concatenate([y_train, y_test])\n",
        "# all_x = np.reshape(all_x, (all_x.shape[0], -1))\n",
        "all_y = all_y.ravel()\n",
        "# all_y = np_utils.to_categorical(all_y)\n",
        "all_y = OneHotEncoder().fit_transform(all_y.reshape(-1, 1)).todense()\n",
        "\n",
        "all_train_x, test_x, all_train_y, test_y = train_test_split(all_x, all_y, \n",
        "                                                            test_size=TEST_OVER_TOTAL)\n",
        "train_x, val_x, train_y, val_y = train_test_split(all_train_x, all_train_y, \n",
        "                                                  test_size=VAL_OVER_TRAIN)\n",
        "\n",
        "num_classes = all_y.shape[1]\n",
        "\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "train_x = scaler.fit_transform(train_x)\n",
        "val_x = scaler.transform(val_x)\n",
        "test_x = scaler.transform(test_x)\n",
        "    \n",
        "# Model architecturse: 3x (dense = 50, dropout) \n",
        "\n",
        "print(\"Full Data, Label shapes = \", all_x.shape, \", \", all_y.shape)\n",
        "print(\"All data finite? \", np.isfinite(all_x).all())\n",
        "print(\"All labels finite? \", np.isfinite(all_y).all())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIWsGjVf82pU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Algorithm & Hyperparameter Selection\n",
        "\n",
        "All sets of hyperparameters in the given dictionary will be run"
      ]
    },
    {
      "metadata": {
        "id": "6igOJcUV7oD1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################\n",
        "# Configurable constant thingy #\n",
        "################################\n",
        "RHC_TRIALS = 1\n",
        "#################################\n",
        "\n",
        "RHC_hyperparams = []\n",
        "RHC_metadict = []\n",
        "\n",
        "# Hyperparams to test\n",
        "RHC_hyperparams = [\n",
        "    # all no better than random?\n",
        "    # {\"max_iters\": 1000, \"learning_rate\": 0.05, \"early_stopping\": True, \"clip_max\": 100, \"max_attempts\": 500000,},\n",
        "    # {\"max_iters\": 1000, \"learning_rate\": 0.1, \"early_stopping\": True, \"clip_max\": 10**10, \"max_attempts\": 100,},\n",
        "    # {\"max_iters\": 1000, \"learning_rate\": 1, \"early_stopping\": True, \"clip_max\": 10**10, \"max_attempts\": 200,},\n",
        "    # {\"max_iters\": 1000, \"learning_rate\": 10, \"early_stopping\": True, \"clip_max\": 10**10, \"max_attempts\": 1000,},\n",
        "]\n",
        "for r_p in RHC_hyperparams:\n",
        "    r_p[\"algorithm\"] = \"random_hill_climb\"\n",
        "    RHC_metadict.append({\"name\": \"RHC\", \"trials\": RHC_TRIALS, \"params\": r_p})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "omFqWAHj9QCn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################\n",
        "# Configurable constant thingy #\n",
        "################################\n",
        "SA_TRIALS = 1\n",
        "#################################\n",
        "\n",
        "SA_hyperparams = []\n",
        "SA_metadict = []\n",
        "\n",
        "# Hyperparams to test\n",
        "SA_hyperparams = [\n",
        "    {\"max_iters\": 2000, \"learning_rate\": 0.01, \"schedule\": mlrose.GeomDecay(1, .99, .01),\n",
        "     \"early_stopping\": True, \"clip_max\": 100, \"max_attempts\": 100,},\n",
        "   #  {\"max_iters\": 2000, \"learning_rate\": 0.01, \"schedule\": mlrose.ExpDecay(1, .01, .01),\n",
        "   #   \"early_stopping\": True, \"clip_max\": 100, \"max_attempts\": 100,},\n",
        "]\n",
        "for s_p in SA_hyperparams:\n",
        "    s_p[\"algorithm\"] = \"simulated_annealing\"\n",
        "    SA_metadict.append({\"name\": \"SA\", \"trials\": SA_TRIALS, \"params\": s_p})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZJ3v6cO1923t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################\n",
        "# Configurable constant thingy #\n",
        "################################\n",
        "GA_TRIALS = 1\n",
        "#################################\n",
        "\n",
        "GA_hyperparams = []\n",
        "GA_metadict = []\n",
        "\n",
        "# Hyperparams to test\n",
        "GA_hyperparams = [\n",
        "    {\"max_iters\": 1000, \"learning_rate\": 0.1, \"pop_size\": 200, \"mutation_prob\": 0.1,\n",
        "     \"early_stopping\": True, \"clip_max\": 5, \"max_attempts\": 100,},\n",
        "    # {\"max_iters\": 1000, \"learning_rate\": 0.1, \"pop_size\": 100, \"mutation_prob\": 0.3,\n",
        "    #  \"early_stopping\": True, \"clip_max\": 5, \"max_attempts\": 100,},\n",
        "    # {\"max_iters\": 1000, \"learning_rate\": 0.1, \"pop_size\": 2000, \"mutation_prob\": 0.2,\n",
        "    #  \"early_stopping\": True, \"clip_max\": 5, \"max_attempts\": 100,},\n",
        "]\n",
        "for g_p in GA_hyperparams:\n",
        "    g_p[\"algorithm\"] = \"genetic_alg\"\n",
        "    GA_metadict.append({\"name\": \"GA\", \"trials\": GA_TRIALS, \"params\": g_p})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cAaX5pGa-RW4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate all sets of hyperparameters and graph losses"
      ]
    },
    {
      "metadata": {
        "id": "jEjpXKhX-vz3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval_algos(train_tuple, val_tuple, algo_specs, architecture=[50, 50, 50], verbose=True):\n",
        "  \n",
        "    def print_v(*args):\n",
        "        if verbose:\n",
        "            print(*args)\n",
        "  \n",
        "    scaler = MinMaxScaler()\n",
        "    \n",
        "    train_x, train_y = train_tuple\n",
        "    val_x, val_y = val_tuple\n",
        "    \n",
        "    acc_list = []\n",
        "\n",
        "    algo_specs = copy.deepcopy(algo_specs)\n",
        "    \n",
        "    for s in algo_specs:\n",
        "        s[\"params\"][\"hidden_nodes\"] = architecture\n",
        "\n",
        "    for index, algo_spec in enumerate(algo_specs):\n",
        "        print_v(\"\\n========================================================\")\n",
        "        print_v(algo_spec[\"name\"] + \" \" + str(index))\n",
        "        print_v(\"Hyperparams \", algo_spec[\"params\"])\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        num_trials = algo_spec[\"trials\"]\n",
        "        train_accuracies = [None] * num_trials\n",
        "        val_accuracies = [None] * num_trials\n",
        "\n",
        "        for trial_index in range(algo_spec[\"trials\"]):\n",
        "            network = mlrose.NeuralNetwork(**algo_spec[\"params\"])\n",
        "            network.fit(train_x, train_y)\n",
        "            \n",
        "            train_predicts = np.nan_to_num(network.predict(train_x))\n",
        "            train_accuracies[trial_index] = accuracy_score(train_predicts, train_y)\n",
        "            val_predicts = np.nan_to_num(network.predict(val_x))\n",
        "            val_accuracies[trial_index] = accuracy_score(val_predicts, val_y)\n",
        "            \n",
        "        acc_list.append([np.average(train_accuracies), np.average(val_accuracies)])\n",
        "\n",
        "        end_time = time.time()\n",
        "        print_v(\"--------------------------------------------------------\")\n",
        "        print_v(\"Total time spent training is \" + str(end_time - start_time) + \" seconds\")\n",
        "        print_v(\"Average train accuracy achieved was \" + str(train_accuracies[index]))\n",
        "        print_v(\"Average val accuracy achieved was \" + str(val_accuracies[index]))\n",
        "        \n",
        "    return acc_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KCnHDDgpDuik",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Actually run the code which hallejuhah\n",
        "c = eval_algos((train_x, train_y), (val_x, val_y), RHC_metadict + SA_metadict + GA_metadict)\n",
        "print(\"C is \", c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQIDUfnVqd0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Graph the performance of any set of hyperparameters vs num iterations"
      ]
    },
    {
      "metadata": {
        "id": "JI59C3YNiXaH",
        "colab_type": "code",
        "outputId": "e1a1b92b-a185-4cc8-b1a3-518abfd945fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "# Iteration vs Time Code\n",
        "size_changer_params = [\n",
        "    # {\"baseline\": RHC_metadict, \n",
        "    #  \"pkey\": \"max_iters\", \"flavor\": \"Training Epochs\",\n",
        "    #  \"tform\": (lambda x: int(x)), \"trials\": 1,\n",
        "    #  \"min_val\": 1, \"max_val\": 501, \"pstep\": 100},\n",
        "    \n",
        "    # {\"baseline\": SA_metadict, \n",
        "    #  \"pkey\": \"max_iters\", \"flavor\": \"Training Epochs\",\n",
        "    #  \"tform\": (lambda x: int(x)), \"trials\": 1,\n",
        "    # \"min_val\": 1, \"max_val\": 501, \"pstep\": 100},\n",
        "    \n",
        "    # {\"baseline\": SA_metadict, \n",
        "    #  \"pkey\": \"max_iters\", \"flavor\": \"Training Iterations\",\n",
        "    #  \"min_val\": 1, \"max_val\": 501, \"pstep\": 100},\n",
        "\n",
        "    {\"baseline\": GA_metadict, \n",
        "     \"pkey\": \"max_iters\", \"flavor\": \"Training Epochs\",\n",
        "     \"tform\": (lambda x: int(x)), \"trials\": 1,\n",
        "    \"min_val\": 1, \"max_val\": 501, \"pstep\": 100},\n",
        "]\n",
        "\n",
        "for index, cp in enumerate(size_changer_params):\n",
        "\n",
        "    param_range = np.arange(cp[\"min_val\"], cp[\"max_val\"], cp[\"pstep\"])\n",
        "    train_accs = [None] * len(param_range)\n",
        "    val_accs = [None] * len(param_range)\n",
        "    for learner_index, pval in enumerate(param_range):\n",
        "        params = copy.deepcopy(cp[\"baseline\"][0])\n",
        "        params[\"params\"][cp[\"pkey\"]] = cp[\"tform\"](pval)\n",
        "        params[\"trials\"] = cp[\"trials\"]\n",
        "        \n",
        "        res = eval_algos((train_x, train_y), (val_x, val_y), [params], verbose=True)\n",
        "        train_accs[index] = res[0]\n",
        "        val_accs[index] = res[0]\n",
        "        \n",
        "    # Graphing the Results!!!\n",
        "    plt.figure(index + 1)\n",
        "    #ax = plt.subplot(len(changer_params), 1, index + 1)\n",
        "    plt.title(cp[\"baseline\"][0][\"name\"] + \": Average Fitness vs \" + cp[\"flavor\"])\n",
        "    plt.xlabel(cp[\"flavor\"])\n",
        "    plt.ylabel('Average Accuracy')\n",
        "    # plt.ylim(ymin=0)\n",
        "    # avg_fitnesses = np.transpose(np.array(avg_fitnesses))\n",
        "    pylab.legend(loc='lower right')\n",
        "    plt.plot(param_range, train_accs, label=\"Train\")\n",
        "    plt.plot(param_range, val_accs, label=\"Val\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "========================================================\n",
            "GA 0\n",
            "Hyperparams  {'max_iters': 1, 'learning_rate': 0.1, 'pop_size': 200, 'mutation_prob': 0.1, 'early_stopping': True, 'clip_max': 5, 'max_attempts': 100, 'algorithm': 'genetic_alg', 'hidden_nodes': [50, 50, 50]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/mlrose/activation.py:98: RuntimeWarning: overflow encountered in exp\n",
            "  fx = np.exp(x)/np.reshape(np.sum(np.exp(x), axis=1), [len(x), 1])\n",
            "/usr/local/lib/python3.6/dist-packages/mlrose/activation.py:98: RuntimeWarning: invalid value encountered in true_divide\n",
            "  fx = np.exp(x)/np.reshape(np.sum(np.exp(x), axis=1), [len(x), 1])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "uGRnDXCp2hDy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# There's some bug with my plotting code above, so I manually plot out accuracies\n",
        "# which are printed by my code during training\n",
        "param_range = np.arange(1, 501, 100)\n",
        "\n",
        "\n",
        "# RHC results\n",
        "# NAME = \"RHC\"\n",
        "# train_accs = np.array([.483, .5039, .495, .505, .4911])\n",
        "# val_accs = np.array([.492, .489, .505, .502, .484])\n",
        "\n",
        "# Simulated Annealing Results\n",
        "NAME = \"SA\"\n",
        "train_accs = np.array([.495, .5039, .497, .497, .49675])\n",
        "val_accs = np.array([.495, .487, .505, .496, .48875])\n",
        "\n",
        "#####################\n",
        "# Graph the things! #\n",
        "#####################\n",
        "\n",
        "plt.title(NAME + \": Accuracy vs Iterations\")\n",
        "plt.xlabel(\"Number training iterations\")\n",
        "plt.ylabel('Accuracy')\n",
        "# plt.ylim(ymin=0)\n",
        "# avg_fitnesses = np.transpose(np.array(avg_fitnesses))\n",
        "plt.plot(param_range, train_accs, label=\"Train\")\n",
        "plt.plot(param_range, val_accs, label=\"Val\")\n",
        "pylab.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}