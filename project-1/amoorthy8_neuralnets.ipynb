{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amoorthy8-neuralnets.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4C3-KUNey2gt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# INSTALL REQUIREMENTS\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "adDYYbOzeRTF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **References**: \n",
        "\n",
        "Most of the IMDB-related code is sourced from [here](https://appliedmachinelearning.blog/2018/02/01/setting-up-deep-learning-in-windows-installing-keras-with-tensorflow-gpu/).\n",
        "\n",
        "The CIFAR-10 related code is sourced from [here](https://github.com/abhijeet3922/Object-recognition-CIFAR-10/blob/master/cifar10.py)"
      ]
    },
    {
      "metadata": {
        "id": "eOQ4mKf_Sj92",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** IMPORTANT NOTE** For whatever reason, the first time you run this code it will fail, complaining about some shape error. Simply run it again and it will function"
      ]
    },
    {
      "metadata": {
        "id": "YNETKCn3hyDy",
        "colab_type": "code",
        "outputId": "4f477eda-c177-47b9-e8a7-9e290cb3e838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "import keras.layers as layers\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.utils import np_utils \n",
        "from keras.datasets import cifar10\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "try:\n",
        "    from google.colab import files\n",
        "except ImportError:\n",
        "  print(\"Not running on colab\")\n",
        "import scipy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "NXpqafWYh6OK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ** Choosing Dataset ** \n",
        "\n",
        "As in the other jupyter notebook, run one of the following two cells depending on which dataset you want to train on. Change the hyperparameter_configs variable if you would like to change the hyperparameters being tested\n",
        "\n",
        "**CIFAR CELL**: Run this cell if you would like to train a neural network on the CIFAR dataset"
      ]
    },
    {
      "metadata": {
        "id": "IlCTszBDeZZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c422b8bf-2761-41d0-fb35-c16bfef23785"
      },
      "cell_type": "code",
      "source": [
        "# CIFAR CELL\n",
        "\n",
        "##########################################\n",
        "# Tweakable parameters (USE THESE A LOT) #\n",
        "##########################################\n",
        "\n",
        "VAL_OVER_TRAIN = .2\n",
        "TEST_OVER_TOTAL = .4\n",
        "\n",
        "PERCENT_DATASET = [.02, .1, .2, .3, .4, .5, .6, .7, .8]\n",
        "\n",
        "hyperparam_configs = [\n",
        "    #{\"epochs\": 10, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 20, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 50, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 100, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 10, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.0},\n",
        "    #{\"epochs\": 20, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.1},\n",
        "    #{\"epochs\": 50, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.3},\n",
        "    #{\"epochs\": 100, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.5},\n",
        "    #{\"epochs\": 10, \"lrate\": .02, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 20, \"lrate\": .02, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 50, \"lrate\": .02, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 100, \"lrate\": .02, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 10, \"lrate\": .05, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 20, \"lrate\": .05, \"batch_size\": 32, \"momentum\": 0.8},\n",
        "    #{\"epochs\": 50, \"lrate\": .05, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    {\"epochs\": 100, \"lrate\": .05, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "]\n",
        "\n",
        "################################################\n",
        "# Loading Data, split into test/train/val sets #\n",
        "################################################\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "x_all, labels = np.concatenate([x_train, x_test]), np.concatenate([y_train, y_test])\n",
        "labels = labels.ravel()\n",
        "\n",
        "X_train_all, X_test, y_train_all, y_test = train_test_split(x_all, labels, \n",
        "                                                    test_size=TEST_OVER_TOTAL)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, \n",
        "                                                     test_size=VAL_OVER_TRAIN)\n",
        "\n",
        "####################\n",
        "# Image Processing #\n",
        "####################\n",
        "\n",
        "K.set_image_dim_ordering('th')\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_valid = X_valid.astype('float32')\n",
        "X_train_all = X_train_all.astype('float32')\n",
        "\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_valid = X_valid / 255.0\n",
        "X_train_all = X_train_all / 255.0\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_train_all = np_utils.to_categorical(y_train_all)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "y_valid = np_utils.to_categorical(y_valid)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "print(\"Shape of training data: \", X_train.shape)\n",
        "print(\"Number of classes: \", y_test.shape[1])\n",
        "\n",
        "##################\n",
        "# MODEL CREATION #\n",
        "##################\n",
        "\n",
        "def create_model(lrate, momentum, batch_size, epochs):\n",
        "  \n",
        "    decay = lrate / epochs\n",
        "    sgd = SGD(lr=lrate, momentum=momentum, decay=decay, nesterov=False)\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32,(3,3), input_shape = (3,32,32), padding = 'same', activation = 'relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv2D(32,(3,3), padding = 'same', activation = 'relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Conv2D(64,(3,3), padding = 'same', activation = 'relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512,activation='relu',kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    \n",
        "    return model, {\"epochs\": epochs, \"batch_size\": batch_size}\n",
        "  \n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 45s 0us/step\n",
            "Shape of training data:  (28800, 32, 32, 3)\n",
            "Number of classes:  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nwsCuFXDxWzJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** IMDB Cell** Run this cell if you would like to train on the IMDB dataset"
      ]
    },
    {
      "metadata": {
        "id": "K76defrqiN6Y",
        "colab_type": "code",
        "outputId": "d2c5e6ee-def4-4fa3-f72b-b959acf3686e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# IMDB CELL\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import os.path\n",
        "ONEHOTS_FILENAME = \"imdb-onehots.gz\"\n",
        "LABELS_FILENAME = \"imdb-labels.gz\"\n",
        "\n",
        "# Changeable parameters\n",
        "MAX_WORD_FEATURES = 10000\n",
        "\n",
        "VAL_OVER_TRAIN = .2\n",
        "TEST_OVER_TOTAL = .4\n",
        "\n",
        "PERCENT_DATASET = [.02, .1, .2, .3, .4, .5, .6, .7, .8]\n",
        "\n",
        "hyperparam_configs = [\n",
        "    #{\"epochs\": 10, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 20, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 50, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 100, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 10, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.0},\n",
        "    #{\"epochs\": 20, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.1},\n",
        "    #{\"epochs\": 50, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.3},\n",
        "    #{\"epochs\": 100, \"lrate\": .01, \"batch_size\": 32, \"momentum\": 0.5},\n",
        "    #{\"epochs\": 10, \"lrate\": .02, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 20, \"lrate\": .02, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 50, \"lrate\": .02, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 100, \"lrate\": .02, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 10, \"lrate\": .05, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    #{\"epochs\": 20, \"lrate\": .05, \"batch_size\": 32, \"momentum\": 0.8},\n",
        "    #{\"epochs\": 50, \"lrate\": .05, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "    {\"epochs\": 100, \"lrate\": .05, \"batch_size\": 32, \"momentum\": 0.9},\n",
        "]\n",
        "\n",
        "def clean_text(raw_review):\n",
        "    # Function to convert a raw review to a string of words\n",
        "    \n",
        "    # Import modules\n",
        "    from bs4 import BeautifulSoup\n",
        "    import re\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem.porter import PorterStemmer\n",
        "    \n",
        "    review_text = BeautifulSoup(raw_review, 'html.parser').get_text() # Remove HTML\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) # Remove non-letters \n",
        "    words = letters_only.lower().split() # Convert to lower case, split into individual words\n",
        "    stops = set(stopwords.words(\"english\")) # Remove stop words (use of sets makes this faster)               \n",
        "    meaningful_words = [w for w in words if not w in stops]                             \n",
        "    porter = PorterStemmer() # Reduce word to stem of word\n",
        "    stemmed_words = [porter.stem(w) for w in meaningful_words]\n",
        "    joined_words = ( \" \".join( stemmed_words )) # Join the words back into one string separated by space\n",
        "    return joined_words \n",
        "\n",
        "def apply_cleaning_function_to_series(X):\n",
        "    print('Cleaning data')\n",
        "    start_time = time.time()\n",
        "    cleaned_X = []\n",
        "    for element in X:\n",
        "        cleaned_X.append(clean_text(element))\n",
        "    print ('Finished in ', str((time.time() - start_time)/60), \" minutes\")\n",
        "    return cleaned_X\n",
        "\n",
        "if not os.path.isfile(ONEHOTS_FILENAME + \".npz\"):\n",
        "    nltk.download('stopwords')\n",
        "    print(\"one-hots not created yet: cleaning and saving to file\")\n",
        "    print(\"Expect this to take about 10-15 minutes\")\n",
        "    data = pd.read_csv('https://gitlab.com/michaelallen1966/00_python_snippets_and_recipes/raw/master/machine_learning/data/IMDb.csv')\n",
        "    \n",
        "    x_cleaned = apply_cleaning_function_to_series(data[\"review\"])\n",
        "    labels = np.array(data[\"sentiment\"]).ravel()\n",
        "    \n",
        "    # Free up memory!\n",
        "    data = None\n",
        "    vectorizer = CountVectorizer(analyzer=\"word\",\n",
        "                                 tokenizer=None,\n",
        "                                 preprocessor=None,\n",
        "                                 stop_words=None,\n",
        "                                 ngram_range=(1,1),\n",
        "                                 max_features=MAX_WORD_FEATURES)\n",
        "    vectorizer.fit(x_cleaned)\n",
        "    x_all = vectorizer.transform(x_cleaned)\n",
        "    x_cleaned = None\n",
        "    scipy.sparse.save_npz(ONEHOTS_FILENAME, x_all)\n",
        "    np.savetxt(LABELS_FILENAME, labels)\n",
        "else:\n",
        "    print(\"loading one-hots from file\")\n",
        "    start_time = time.time()\n",
        "    x_all = scipy.sparse.load_npz(ONEHOTS_FILENAME + \".npz\")\n",
        "    labels = np.loadtxt(LABELS_FILENAME)\n",
        "    end_time = time.time()\n",
        "    print(\"Finished loading one-hots in \", (end_time - start_time)/60, \" minutes\")\n",
        "    \n",
        "\n",
        "labels = labels.ravel()\n",
        "X_train_all, X_test, y_train_all, y_test = train_test_split(x_all, labels, \n",
        "                                                    test_size=TEST_OVER_TOTAL)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, \n",
        "                                                     test_size=VAL_OVER_TRAIN)\n",
        "    \n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_valid = X_valid.astype('float32')\n",
        "X_train_all = X_train_all.astype('float32')\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_train_all = np_utils.to_categorical(y_train_all)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "y_valid = np_utils.to_categorical(y_valid)\n",
        "num_classes = y_test.shape[1]\n",
        "    \n",
        "def create_model(lrate, momentum, batch_size, epochs):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000,)))\n",
        "    # Hidden - Layers\n",
        "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(50, activation = \"relu\"))\n",
        "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "    model.add(layers.Dense(50, activation = \"relu\"))\n",
        "    # Output- Layer\n",
        "    model.add(layers.Dense(num_classes, activation = \"sigmoid\"))\n",
        "    #model.summary()\n",
        "              \n",
        "    decay = lrate / epochs\n",
        "    sgd = SGD(lr=lrate, momentum=momentum, decay=decay, nesterov=False)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    return model, {\"epochs\": epochs, \"batch_size\": batch_size}    \n",
        "\n",
        "print(\"Full Data, Label shapes = \", x_all.shape, \", \", labels.shape)\n",
        "\n",
        "# files.download(ONEHOTS_FILENAME + \".npz\")\n",
        "# files.download(LABELS_FILENAME)\n",
        "# files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading one-hots from file\n",
            "Finished loading one-hots in  0.006836116313934326  minutes\n",
            "Full Data, Label shapes =  (50000, 10000) ,  (50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GC8cnFXGiVae",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Neural Network Training ** After having run the cell to select the relevant dataset, run the following cells to train neural networks on the selected\n"
      ]
    },
    {
      "metadata": {
        "id": "OFWj3aZOhu0e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################################\n",
        "# EVALUATION OF MODELS AND STUFF #\n",
        "##################################\n",
        "\n",
        "print(\"TEST_OVER_TOTAL\", TEST_OVER_TOTAL)\n",
        "print(\"VAL_OVER_TRAIN\", VAL_OVER_TRAIN)\n",
        "\n",
        "#############################################\n",
        "# MODEL CREATION FUNCTION, INTEGRATED LATER #\n",
        "#############################################\n",
        "\n",
        "def batch_generator(X, y, batch_size, shuffle):\n",
        "    number_of_batches = np.floor(X.shape[0]/batch_size)\n",
        "    counter = 0\n",
        "    sample_index = np.arange(X.shape[0])\n",
        "    if shuffle:\n",
        "        np.random.shuffle(sample_index)\n",
        "    while True:\n",
        "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
        "        X_batch = X[batch_index,:].toarray()\n",
        "        y_batch = y[batch_index]\n",
        "        counter += 1\n",
        "        yield X_batch, y_batch\n",
        "        if (counter == number_of_batches):\n",
        "            if shuffle:\n",
        "                np.random.shuffle(sample_index)\n",
        "            counter = 0\n",
        "\n",
        "model_list = [None] * len(hyperparam_configs)\n",
        "model_history_list = [None] * len(hyperparam_configs)\n",
        "val_score_list = [None] * len(hyperparam_configs)\n",
        "\n",
        "for index, hyperparams in enumerate(hyperparam_configs):\n",
        "  \n",
        "    print(\"\\n==============================================\")\n",
        "    print(\"Beginning to consider model \", index)\n",
        "    print(\"Hyperparams: \", hyperparams)\n",
        "    model, fit_args = create_model(**hyperparams)\n",
        "    \n",
        "    batch_size = fit_args[\"batch_size\"]\n",
        "    num_batches_per_epoch = np.floor(X_train.shape[0] / batch_size)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    #history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), \n",
        "    #                    verbose=1, **fit_args)\n",
        "    history = model.fit_generator(batch_generator(X_train, y_train, batch_size, True),\n",
        "                                  validation_data=(X_valid, y_valid),\n",
        "                                  steps_per_epoch=num_batches_per_epoch,\n",
        "                                  epochs=fit_args[\"epochs\"], \n",
        "                                  verbose=0)\n",
        "    end_time = time.time()\n",
        "    print(\"Finished in \", (end_time - start_time)/60, \" mins\")\n",
        "    \n",
        "    scores = model.evaluate(X_valid, y_valid,verbose=0)\n",
        "    print(\"Final train accuracy of \", history.history['acc'][-1])\n",
        "    print(\"Final validation accuracy of \", scores)\n",
        "    \n",
        "    model_list[index] = model\n",
        "    val_score_list[index] = scores[1]\n",
        "    model_history_list[index] = history\n",
        "    \n",
        "print(\"Final Validation accuracies\")\n",
        "print(val_score_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MgFI967eovVy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Runtime Diagnostics**"
      ]
    },
    {
      "metadata": {
        "id": "798l1y2xolod",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_model_index = np.argmax(val_score_list)\n",
        "history = model_history_list[best_model_index]\n",
        "best_model = model_list[best_model_index]\n",
        "best_hyperparams = hyperparam_configs[best_model_index]\n",
        "\n",
        "print(\"Best model is index \", best_model_index)\n",
        "print(\"Hyperparams for the best model were \", hyperparam_configs[best_model_index])\n",
        "\n",
        "start_time = time.time()\n",
        "test_score = model.evaluate(X_test, y_test,verbose=0)\n",
        "end_time = time.time()\n",
        "print(\"This model achieves a test accuracy of \", test_score)\n",
        "print(\"Evaluation on the test set ran in \", (end_time - start_time)/60, \" mins\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v4poeP3QpjIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Graph accuracy of best model over time\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Best model accuracy vs epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.ylim(ymin=0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnuHtBGsqiBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Graph loss history over time\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Best model loss vs epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.ylim(ymin=0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bmR4evZYru1K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Accuracy vs Dataset Size evaluation**: Finally, we train the model on different fractions of the (train) dataset and observe performance on the test dataset"
      ]
    },
    {
      "metadata": {
        "id": "HPg6UqoYsKWd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# PERCENT_DATASET = [.02, .05]\n",
        "\n",
        "percent_models = [None] * len(PERCENT_DATASET)\n",
        "percent_model_histories = [None] * len(PERCENT_DATASET)\n",
        "\n",
        "pm_final_train_accs = [None] * len(PERCENT_DATASET)\n",
        "pm_final_test_accs = [None] * len(PERCENT_DATASET)\n",
        "\n",
        "for index, percent_of_data in enumerate(PERCENT_DATASET):\n",
        "\n",
        "    print(\"\\n==============================================\")\n",
        "    print(\"Dataset percentage: \", percent_of_data)\n",
        "    model, fit_args = create_model(**best_hyperparams)\n",
        "    \n",
        "    X_train_fractional, t_val, y_train_fractional, t_valor = train_test_split(X_train_all, y_train_all, \n",
        "                                                                             test_size=1-percent_of_data)\n",
        "    \n",
        "    print(X_train.shape)\n",
        "    print(X_train_fractional.shape)\n",
        "\n",
        "    start_time = time.time()\n",
        "    model, fit_args = create_model(**best_hyperparams)\n",
        "\n",
        "    history = model.fit(X_train_fractional, y_train_fractional, \n",
        "                        verbose=0,\n",
        "                        validation_data=(X_valid, y_valid), **fit_args)\n",
        "    end_time = time.time()\n",
        "    print(\"Finished in \", (end_time - start_time)/60, \" mins\")\n",
        "\n",
        "    train_acc = history.history['acc'][-1]\n",
        "    test_score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    pm_final_train_accs[index] = train_acc\n",
        "    pm_final_test_accs[index] = test_score[1]\n",
        "    \n",
        "    print(\"Final Train accuracy of \", train_acc)\n",
        "    print(\"FInal test score of \", test_score)\n",
        "\n",
        "    percent_models[index] = model\n",
        "    percent_model_histories[index] = history\n",
        "    \n",
        "print(\"Final train accs: \", pm_final_train_accs)\n",
        "print(\"Final test accs: \", pm_final_test_accs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ib-kN_JXvMfP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Graph the performance over fraction of dataset used\n",
        "plt.title(\"Accuracy vs Fraction of dataset\")\n",
        "plt.xlabel('Fraction of dataset used')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "# plt.ylim(ymin=0)\n",
        "plt.plot(PERCENT_DATASET, pm_final_train_accs, label=\"Train data\")\n",
        "plt.plot(PERCENT_DATASET, pm_final_test_accs, label=\"Test data\")\n",
        "plt.ylim(ymin=0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}